{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8310919,"sourceType":"datasetVersion","datasetId":4936026},{"sourceId":8315543,"sourceType":"datasetVersion","datasetId":4939499},{"sourceId":8315579,"sourceType":"datasetVersion","datasetId":4938165},{"sourceId":8316204,"sourceType":"datasetVersion","datasetId":4939944}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport time\nimport random\nimport requests\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torchvision.utils\nimport torch.nn.functional as F\nimport torchvision.transforms as transforms\nfrom torchvision import datasets\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\n\nfrom typing import Tuple\n\nimport json\nfrom PIL import Image\n\nimport io\nfrom tqdm.notebook import tqdm\n\n\nseed = 123\nnp.random.seed(seed)\n_ = torch.manual_seed(seed)\n_ = torch.cuda.manual_seed(seed)\n\n# we select to work on GPU if it is available in the machine, otherwise will run on CPU\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\nhparams = {\n    'batch_size':64,\n    'num_epochs':8,\n    'channels':32,\n    'latent_dims':64,\n    'variational_beta':1,\n    'learning_rate':1e-3,\n    'weight_decay':1e-5\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"folder_path = \"/kaggle/input/masked10k/masked_images/\"\nbatch_size = hparams[\"batch_size\"]\n\nimages = [folder_path + im for im in os.listdir(folder_path) if im.endswith(\".jpg\")]\n\nn_samples = len(images)\nprint(f\"{n_samples} images\")\n\n# indices for all time steps where the episode continues\nindices = np.arange(n_samples, dtype=\"int64\")\nnp.random.shuffle(indices)\n\n# split indices into minibatches. minibatchlist is a list of lists; each\n# list is the id of the observation preserved through the training\nminibatchlist = [\n    np.array(sorted(indices[start_idx : start_idx + batch_size]))\n    for start_idx in range(0, len(indices) - batch_size + 1, batch_size)\n]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we define the Autoencoder class and all its components\nclass Encoder(nn.Module):\n    def __init__(\n            self,\n            channels: int,\n            latent_dims: int,\n            ) -> None:\n\n        super(Encoder, self).__init__()\n\n        self.c = channels\n        self.bnorm1 = nn.BatchNorm2d(3)\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=self.c, kernel_size=4, stride=2, padding=1)\n        self.bnorm2 = nn.BatchNorm2d(self.c)\n        self.conv2 = nn.Conv2d(in_channels=self.c, out_channels=2*self.c, kernel_size=4, stride=2, padding=1)\n        self.bnorm3 = nn.BatchNorm2d(2*self.c)\n        self.conv3 = nn.Conv2d(in_channels=2*self.c, out_channels=3*self.c, kernel_size=4, stride=2, padding=1)\n        self.bnorm4 = nn.BatchNorm2d(3*self.c)\n        self.conv4 = nn.Conv2d(in_channels=3*self.c, out_channels=4*self.c, kernel_size=4, stride=2, padding=1)\n        self.fc_mu = nn.Linear(in_features=self.c*4*8*8, out_features=latent_dims)\n        self.fc_logvar = nn.Linear(in_features=self.c*4*8*8, out_features=latent_dims)\n\n    def forward(self, x: torch.Tensor)-> Tuple[torch.Tensor, torch.Tensor]:\n\n        out = F.relu(self.conv1(self.bnorm1(x))) # Batch x channels x 64 x 64\n        out = F.relu(self.conv2(self.bnorm2(out))) # Batch x 2*channels x 32 x 32\n        out = F.relu(self.conv3(self.bnorm3(out))) # Batch x 3*channels x 16 x 16\n        out = F.relu(self.conv4(self.bnorm4(out))) # Batch x 4*channels x 8 x 8\n        out = out.view(out.size(0), -1) # flatten batch of multi-channel feature maps to a batch of feature vectors\n\n        # We obtain the mean and covariance matrices from the output of the linear layers\n        x_mu = self.fc_mu(out)\n        x_logvar = self.fc_logvar(out)\n\n        return x_mu, x_logvar\n\n    \nclass Decoder(nn.Module):\n    def __init__(\n            self,\n            channels: int,\n            latent_dims: int\n            ) -> None:\n\n        super(Decoder, self).__init__()\n        self.c = channels\n        self.fc = nn.Linear(in_features=latent_dims, out_features=self.c*4*8*8)\n        self.conv4 = nn.ConvTranspose2d(in_channels=4*self.c, out_channels=3*self.c, kernel_size=4, stride=2, padding=1)\n        self.bnorm4 = nn.BatchNorm2d(4*self.c)\n        self.conv3 = nn.ConvTranspose2d(in_channels=3*self.c, out_channels=2*self.c, kernel_size=4, stride=2, padding=1)\n        self.bnorm3 = nn.BatchNorm2d(3*self.c)\n        self.conv2 = nn.ConvTranspose2d(in_channels=self.c*2, out_channels=self.c, kernel_size=4, stride=2, padding=1)\n        self.bnorm2 = nn.BatchNorm2d(self.c*2)\n        self.conv1 = nn.ConvTranspose2d(in_channels=self.c, out_channels=3, kernel_size=4, stride=2, padding=1)\n        self.bnorm1 = nn.BatchNorm2d(self.c)\n\n    def forward(self, z: torch.Tensor) -> torch.Tensor:\n\n        out = self.fc(z)\n        out = out.view(out.size(0), self.c*4, 8, 8) # unflatten batch\n\n        out = F.relu(self.conv4(self.bnorm4(out)))\n        out = F.relu(self.conv3(self.bnorm3(out)))\n        out = F.relu(self.conv2(self.bnorm2(out)))\n        out = torch.sigmoid(self.conv1(self.bnorm1(out)))\n        return out\n    \n    \nclass VariationalAutoencoder(nn.Module):\n    def __init__(\n            self,\n            z_dims: int,\n            n_ch: int,\n            ) -> None:\n\n        super(VariationalAutoencoder, self).__init__()\n        self.encoder = Encoder(channels=n_ch, latent_dims=z_dims)\n        self.decoder = Decoder(channels=n_ch, latent_dims=z_dims)\n\n    def reparametrize(\n            self,\n            mu:torch.Tensor,\n            logvar:torch.Tensor,\n            ) -> torch.Tensor:\n        # Given mean and logvar returns z\n        # reparameterization trick: instead of sampling from Q(z|X), sample epsilon = N(0,I)\n        # mu, logvar: mean and log of variance of Q(z|X)\n\n        # The factor 1/2 in the exponent ensures that the distribution has unit variance\n        std = torch.exp(0.5 * logvar)\n        # Random sample\n        eps = torch.randn_like(std)\n        return mu + std * eps\n\n    def forward(self, x: torch.Tensor) -> Tuple[\n            torch.Tensor, torch.Tensor, torch.Tensor]:\n        latent_mu, latent_logvar = self.encoder(x)\n        z = self.reparametrize(latent_mu, latent_logvar)\n        x_recon = self.decoder(z)\n\n        return x_recon, latent_mu, latent_logvar\n\n    \ndef vae_loss(\n        recon_x: torch.Tensor,\n        x: torch.Tensor,\n        mu: torch.Tensor,\n        logvar: torch.Tensor,\n        variational_beta: int=1,\n        ) -> float:\n    # recon_x is the probability of a multivariate Bernoulli distribution p.\n    # -log(p(x)) is then the pixel-wise binary cross-entropy.\n\n    recon_loss = F.binary_cross_entropy(recon_x.view(-1, 393216), x.view(-1, 393216), reduction='sum')\n    kldivergence = variational_beta * (-0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()))\n    mean_batch_loss = (recon_loss +  kldivergence)/x.shape[0]\n\n    return mean_batch_loss","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_batch(\n        image_batch: torch.Tensor,\n        vae: torch.nn.Module,\n        vae_loss: torch.nn.Module,\n        optimizer: torch.optim,\n        ) -> float:\n\n    image_batch = image_batch.to(device)\n\n    # Get vae reconstruction and loss\n    image_batch_recon, latent_mu, latent_logvar = vae(image_batch)\n    loss = vae_loss(image_batch_recon, image_batch, latent_mu, latent_logvar)\n\n    # backpropagation\n    optimizer.zero_grad()\n    loss.backward()\n\n    # one step of the optmizer (using the gradients from backpropagation)\n    optimizer.step()\n\n    return loss.item()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transf = transforms.Compose([\n        transforms.Resize((128, 128), Image.BICUBIC),\n        transforms.ToTensor(),\n    ])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start_time=time.time()\n\n# Instantiation of optimizer and model\nvae_2z = VariationalAutoencoder(hparams['latent_dims'], hparams['channels']).to(device)\noptimizer = torch.optim.Adam(params=vae_2z.parameters(), lr=hparams['learning_rate'], weight_decay=hparams['weight_decay'])\n\n# Number of parameters used in the model\nnum_params = sum(p.numel() for p in vae_2z.parameters() if p.requires_grad)\nprint(f'Number of parameters: {num_params}')\n\n# set to training mode\nvae_2z.train()\n\ntrain_loss_avg = []\n\nprint('Training ...')\nfor epoch in range(hparams['num_epochs']):\n    print(f\"Epoch {epoch}\")\n    train_loss_avg.append(0)\n    num_batches = 0\n    print(f\"Minibatchlist size: {len(minibatchlist)}\")\n    for i,image_batch in tqdm(enumerate(minibatchlist)):\n        img_batch = None\n        for item in image_batch:\n            try:\n                if img_batch is None:\n                    img_batch = transf(Image.open(images[item]).convert(\"RGB\")).unsqueeze(0)\n                else:\n                    img_batch = torch.cat((img_batch,transf(Image.open(images[item]).convert(\"RGB\")).unsqueeze(0)),0)\n            except:\n                done = False\n                while not done:\n                    try:\n                        idx = random.randint(0,len(images)-1)\n                        if img_batch is None:\n                            img_batch = transf(Image.open(images[idx]).convert(\"RGB\")).unsqueeze(0)\n                        else:\n                            img_batch = torch.cat((img_batch,transf(Image.open(images[idx]).convert(\"RGB\")).unsqueeze(0)),0)\n                        done = True\n                    except:\n                        pass\n        loss_batch = train_batch(img_batch, vae_2z, vae_loss, optimizer)\n        train_loss_avg[-1] += loss_batch\n\n    train_loss_avg[-1] /= i\n    print('Epoch [%d / %d] average reconstruction error: %f' % (epoch+1, hparams['num_epochs'], train_loss_avg[-1]))\n    \n    trans = torchvision.transforms.ToPILImage()\n    inp = transf(Image.open(images[0])).unsqueeze(0).to(device)\n    display(trans(inp.squeeze()))\n    x,_,_ = vae_2z(inp)\n    out = trans(x.squeeze())\n    display(out)\n    print(\"--- ELAPSED TIME: %s min ---\" % (round((time.time() - start_time) / 60, 3)))\n\nprint(\"--- TOTAL TIME: %s min ---\" % (round((time.time() - start_time) / 60, 3)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### We save the trained model","metadata":{}},{"cell_type":"code","source":"m = torch.jit.script(vae_2z)\n# Save to file\ntorch.jit.save(m, 'model.pt')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### We load the saved models","metadata":{}},{"cell_type":"code","source":"model_path1 = \"/kaggle/input/mod-mask/model_mask.pt\"\nmod1 = torch.jit.load(model_path1).to(device)\n\nmodel_path2 = \"/kaggle/input/modelo/model.pt\"\nmod2 = torch.jit.load(model_path2).to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### We check the distance between the embeddings of some images","metadata":{}},{"cell_type":"code","source":"folder_path = \"/kaggle/input/hackupc/Imatges/\"\n\nimages = [folder_path + im for im in os.listdir(folder_path) if im.endswith(\".jpg\")]\n\ntrans = torchvision.transforms.ToPILImage()\ndist = nn.PairwiseDistance()\n\ndef display_vae_reconstr(mod, inp):\n    display(trans(inp.squeeze()))\n    x,_,_ = mod(inp)\n    out = trans(x.squeeze())\n    display(out)\n    \ndef find_closer(mod,inp,chosen_i):\n    latent_mu, latent_logvar = mod.encoder(inp)\n    z = mod.reparametrize(latent_mu, latent_logvar)\n    min_dist = 100000\n    min_z = 0\n    for i in range(1000):\n        try:\n            inp2 = transf(Image.open(images[i])).unsqueeze(0).to(device)\n            latent_mu, latent_logvar = mod.encoder(inp2)\n            z2 = mod.reparametrize(latent_mu, latent_logvar)\n            if dist(z,z2) < min_dist and i != chosen_i:\n                min_dist = dist(z,z2)\n                min_index = i\n        except:\n            pass\n    return min_index, min_dist\n\nchosen_i = 244\ninp = transf(Image.open(images[chosen_i])).unsqueeze(0).to(device)\ndisplay_vae_reconstr(mod2, inp)\n\ni1,_ = find_closer(mod2,inp,chosen_i)\n\ninp_mod1 = transf(Image.open(images[i1])).unsqueeze(0).to(device)\n\ndisplay_vae_reconstr(mod2, inp_mod1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Saving the embeddings of every image","metadata":{}},{"cell_type":"code","source":"for i,img in enumerate(images):\n    try:\n        inp = transf(Image.open(img)).unsqueeze(0).to(device)\n        latent_mu, latent_logvar = mod2.encoder(inp)\n        z = mod2.reparametrize(latent_mu, latent_logvar)\n        col = torch.flatten(z.cpu()).detach().numpy()\n        if i == 0:\n            res = pd.DataFrame(col, columns=[\"embedding_1\"])\n        else:\n            colname = \"embedding_\"+str(i+1)\n            res[colname] = col\n    except:\n        pass\n\nres.to_csv('emb_vae.csv', index=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}